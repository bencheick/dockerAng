M2 - Managing images using Docker Hub
If we want to make full use of Docker's various image-management tools, we'll need a bit more insight into the basics of the way the Docker image ecosystem is designed: how images are pulled from and pushed to remote repositories, how you build your own images, and what is the stuff of which images are actually made. But first, I'll talk a bit about why images are so important to Docker and how that relationship shapes the way we build them.
Among the key features that attract so much attention to Docker are speed and, what I'll call predictability...both of which owe a lot to the core design of the Docker image.
A great deal of Docker's speed is due to the way images are divided into independent layers, each of which can actually be shared among multiple images. Among other things, this means that, depending on what you've already got in your local storage area, pulling new images from remote repos can sometimes require downloading very little new data. We'll talk more about how layers are added to an image later on.
Now what about predictability? Docker lets you launch containers that provide identical environments no matter where they're hosted and no matter how many times they've been launched before. This can be really useful if you're sharing your application with colleagues working remotely, or if you're deploying your application live to production. Either way, you have a keen interest in knowing that all the code and software and system dependencies are there, exactly the way they were when you created them. 
Basing your environment on Docker images is way better than shipping your applications with painstakingly documented instructions on how it should be installed and deployed, or even working from snapshots of virtual machines; a Docker image is the documentation and, with a single command, can become the machine.
So, by "predictable", I mean that a Docker image will reliably deliver exactly what you expect it to. That is, you can be confident that nothing affecting your container's behaviour will change between leaving your workstation and reaching the rest of the world.
We'll talk more about what you have to do to ensure this happens a bit later in this module. Now, however, it's time to work through some of the basics. 

In this demo, I'm going to show you how to find out what images are available both locally, and on the Docker Hub, how to peek inside an image to learn about the layers of which it's made, how to write a dockerfile that precisely defines a new image and how to build the image from the dockerfile and confirm it worked out the way we wanted by running it as a container.
You list the images that are currently on your local system with "docker images". There's a lot of output here, so I'll put it on a slide. We're shown the repository name, tag, image ID, history, and size. "Docker search" will look through Docker's primary online repository - that's Docker Hub - for freely available public images whose names and descriptions match your search terms. My search for an image based on the Ubuntu operating system with the Go programming language environment pre-installed returned a nice variety of results, including an official image which, as you can see, has all kinds of star ratings. Those two points are important as you really want to avoid taking chances with shady images that could have malware embedded. So remember: official is good.
When you find the image of your dreams, you can incorporate it into your system either by referencing it by name in some kind of script - like a dockerfile - or by pulling it directly from the command line. I'll pull what's got to be one of the smallest operating systems known to mankind: Alpine Linux - the bare-bones, security-conscious distribution that's often used as the base for Docker images. 
"Docker images" shows us that Alpine is there...and just how small it is. Less than 4MB! I'll run "docker history" to take a look at what's inside. Now, I'll admit that in this case, history tells us very little that we hadn't already seen using docker images. But there is one item of interest: the "created by" column, which appears to be an invocation of the sh shell that runs an ADD operation against that file. 
Let me illustrate what's going on by running history again, but this time against the ubuntu image that's already on my system. This one has a half a dozen lines, the file that was the target of this ADD operation at the bottom is a whole lot bigger than the one we saw with Alpine. But then, you'd expect that, considering how much more functionality comes with Ubuntu. The "missing" image names is nothing to worry about...it's a product of the recent transition to the new content addressable storage model adopted with Docker version 1.10.
Let's build our own image. I've written this simple dockerfile with all the ingredients needed to create an image of an Apache webserver on Ubuntu. The FROM line tells Docker that the image will be based on the Ubuntu 16.04 image, which will take up the first layer of our new image. The two RUN lines will execute apt-get update and apt-get install to install Apache. I use apt-get rather than the newer apt, as apt is potentially less reliable in a scripted setting. ADD will copy a file I'm going to create in the current directory called index.html to the web root directory, /var/www/html. This will make the file available to users browsing to the web server URL. Of course, I could have saved the file to any spot on the file system. The CMD line makes sure that Apache will actually remain running when the image is launched as a container, and EXPOSE 80 will open up port 80 for web browsers.
I'll use the nano text editor to create a one-line inspirational welcome page for my site, save it using CTRL+x and s, and then run docker build to create the image. -t associates the tag "webserver" to the image.
Running "docker images" once again should show us our new image. I can run a container based on the image using "docker run" and the image name (webserver). The -d tells Docker that I want to run the container detached - meaning, I don't want to open a shell within the container. By default, the container will launch into the bridge network, so I'll run network inspect bridge to get the container's IP address. Here it in in the 172.17.0.1 subnet. now I'll run curl against that address to see what kind of web server we're running...looks like a nice and friendly web server, based on the index.html page. "Docker ps" shows us our running container. However, since being friendly will only get you so far in the dog-eat-dog world of IT administration, I'll use "docker stop" to shut the container down.
Well, we've created our own image based on a dockerfile script, successfully run it as a container, and shut it down. One more thing and we're done with our "Docker Images 101" section: pushing our new image to a Docker Hub repository so I can share it with my friends and family - although, while it is cheaper than flowers, I'm not at all sure what my wife would do with it. I already have a free Docker Hub account, so I only need to run docker login to authenticate and connect with the repo. Let's run "docker image" once again to see our image details. Now I'll give the image a new repository name using docker tag, the current repo name (webserver) and a new name based on my Docker Hub account name. This tag, by the way, has nothing to do with the existing "latest" or 16.04" tags you see in the docker images output. With that, we can use docker push to upload the repository to Docker Hub. Once that's done, I can prove it worked from my repo page on hub.docker.com.

If you want to make the most of Docker's predictability and speed, you'll need to adapt your operation to match some peculiarities of the Docker system. First of all, you'll want to create images that are as stable and reliable as possible. That means being very careful to avoid contaminating them with - well - unpredictability. 
So, in general, you won't want to create an image from a running container using docker commit. There's nothing stopping you from doing that, by the way. I've got a container running based on Ubuntu as we speak. I'll use ps to get its ID, and then run commit against the ID followed by a name I'd like to give the new image. It will work. But it's a bad idea. Why? Because once you've been playing around for a while, you have no way of being absolutely sure what file system or configuration changes have been applied and you'll be unable to confidently reproduce the image. Building your images cleanly from dockerfiles, on the other hand, gives you total control.
Similarly, you can control your images by carefully managing which base images you use in the FROM line in your dockerfiles. This might sometimes require you to ignore what's normally a fundamental principle of responsible administration: always go with the latest patched version. Docker certainly lets you do that, usually by adding a colon and the word "latest" to the distribution name for pull commands or FROM lines in your dockerfile. But, when you're managing dev and production cycles, that can sometimes end up causing you some serious discomfort.
The problem is that, even though the latest version is likely to be the most secure, it won't necessarily be completely compatible with the applications you've got running on it. Once the upstream developers have rolled over to a new version, a dockerfile line reading FROM centos:latest will deliver a different software stack that could unexpectedly break your application. 
The solution is to hard code the release version you want into your dockerfile or pull command by specifying it by number. This example will pull CentOS 6.6. Of course, you'll have to deal with the possible security holes that you might be opening up, but life is all about compromise, isn't it?
A lot of Docker's speed is achieved by keeping images small. Docker Engine does its bit by sharing locally-stored layers between images wherever possible and through some really smart caching. But no one's exempt from doing their part in the Battle of the Bulge. 
I'm going to demonstrate something that I first saw on the Red Hat developers blog. It gets to the core of how Docker images are built. Each new RUN command in a dockerfile will generate a new layer in the resulting image, and each new layer takes up extra space. Therefore, squeezing multiple commands into a single RUN line by using two "and" characters will actually make a big difference. Well, I'm not sure that that's always true. I ran a few of my own experiments and there were cases where the differences were negligible. But here's one where there's no doubting the results. 
I'm going to create two versions of the dockerfile, build images from both, and then compare their sizes. First I'll create a new directory - which allows me to use a new dockerfile without messing with any others I might have already created. Now I'll paste the "multiple line" version. We start with Ubuntu 16.04 as our base and then use apt-get to update the repository index and then install curl - which we'll need for the next step. Now the next commands actually make no sense, as it assumes that I already have a version of JBoss' WildFly application server running on this machine - which isn't true. But the JBoss download happened to be the example from the Red Hat blog, and it still illustrates the point perfectly well, so I'll stick with it. But still, why should I actually use mkdir create a jboss/wildfly directory tree, you might ask. That's because every extra step - no matter how trivial - can become an extra layer.
Just to explain what these commands do: mkdir creates the directory tree I just mentioned into which we'll eventually move the program files we will extract. Next I'll change directory to /tmp and, using curl, download the latest version of WildFly. Then I'll use tar to extract the archive, move it to its new home in /opt/jboss/wildfly, and finally remove the original archive. I'll save the dockerfile and run docker build, tagging the image manylines. The dot at the end, by the way, tells Docker to look for a dockerfile script in the current directory. I'll pause the video while Docker goes about its business, but I will note that Docker's use of existing layers and cache means that nothing will be downloaded unless absolutely necessary. 

Now I'll edit the docker file and, by adding & characters, combine many of the commands into a single RUN line. If I messed something up and corrupted a command, it will "exit with a code other than zero" and the Docker build operation will fail, reporting the error as part of the command output. Either way, I'll save the file and run docker build again, this time tagging the image as oneline.
With that done, I'll run docker images to compare the image sizes. To make it easier to read, I pasted the output to this slide. And the difference is amazing: just by including all the commands in a single RUN line cut the file size nearly in half! So the way we write our dockerfiles can have a huge impact on size...and performance. Before we go on, I'll just delete the two images we just created using rmi.

As that blog post pointed out, this principle can apply to other cases. For instance, if you had the choice of basing an image on the latest version of a distribution like CentOS, or an earlier version, but then updating within your dockerfile, you really want to go with the latest release. That's because invoking yum update (or dnf update as it's more often done now) will add a significant size overhead to your image.

It's review time. You didn't think you'd be getting off so easy, did you? We talked about how so much of the Docker advantage hangs on two principles: speed and predictability - and noted how important it was to build images with those two things in mind. We used docker search to find out what useful images were available on Docker Hub, and docker pull to download an image to our local storage. Docker history gave us an ultrasound view of the insides of an image. We wrote a dockerfile script as a template for building an image that specified a base image and added a local file to the image's web root directory. I then pushed an image up to my Docker Hub account and took a look.
Finally, to demonstrate how you can control the size of our images, I created images from two separate dockerfiles, one with separate RUN lines for each system command, and the other with all the commands compressed into a single line.
In the next module, I'll install and operate my own private Docker repository using Docker Registry. Don't miss it.


