m5 - Other Image Managing Tools
In this final module, I'll introduce you to a couple more important tools for managing Docker images. We'll begin by returning to the subject of security - which, in the IT world at least, is a subject you can never really leave. In particular, I'll demonstrate how to enable Docker Content Trust to validate images as they move between hosts. And then we'll take a brief tour of the commercial administration tool we mentioned a while back: Docker Cloud.
Digital signing is a way for people at two ends of a data transmission to know that the transfer has not been intercepted or corrupted. When both the sender and the recipient possess matching encryption keys, metadata - like hashes - associated with files can be exchanged and verified against known copies. When a client wishes to push a new image to a trusted registry, the image must first be signed using the client's private key. Anyone with the matching public key will subsequently be able to confirm the integrity of an image copy by decrypting the signature. Key and signature administration is handled by Docker Content Trust through the Notary utility. 
However, from a client's perspective, most steps of the process are managed for them invisibly by Docker Engine. Besides having to authenticate with a passphrase, all other command line tools will work exactly the same way as before...with one small caveat: Docker Content Trust won't work at all on a client until you enable it. All that's needed for that to happen is to export the appropriate environment variable to your system. Here's how you slay that dragon:
To permanently enable Content Trust, simply add the "export DOCKER_CONTENT_TRUST=1" line to your /etc/profile file. While we're there anyway, it's always a good idea to add a comment line describing what we're doing. Save the file with CTRL+x and...apparently nothing at all has changed. That's because /etc/profile is only read at the start of a new shell session. So you could now either log out and then log in again, or simply export the value from the command line to this specific shell session. Echo shows us that, this time, we're set. That should keep us happy until we log in again.
Let's try it out. I'll try to pull a unsigned image of Redis in a repo called Xataz - I really hope I pronounced that correctly. I don't imagine that there's really anything wrong with this image, by the way, I'm just using it as an example of how things would work when data hasn't been signed. So let's fire off a request and see if anything blows up. Aha! we're getting a "remote trust data does not exist..." error. 
Now if I were to disable Content Trust by exporting it with a value of 0, and then try pulling the image again, it will work. Docker puts me in the driver's seat all the way. You should be aware that this protection is not only active for command line requests, but also for FROM entries within dockerfiles.
That's pulling. How about pushing a new image up to a repo? Well, why not try it for ourselves? I'm going to log in to Docker Hub, and then give a new tag to the busybox image I've already got available locally that will point the image up to my own Docker Hub repository. 
As you can see, I'm prompted for a root key passphrase. This is because I've previously pushed images from another content-trust-enabled machine and was prompted at that time to create a passphrase. I should also note that the first time you log in, a root key file is created and saved to the ~/.docker/trust/private/root_keys/ directory. If you expect to work from multiple machines - especially if you use VMs a lot - then you should save the contents of that root key - and of your root passphrase - in a secure password vault of some sort. To make this work, obviously, I previously copied my root key file from my password vault and recreated it in the root_keys directory of this VM.
In any case, I'm going to paste my root passphrase in at the prompt and then enter a passphrase for the new repository key. Once that's in, Docker Engine and Content Trust will do the rest.

While Notary will usually act as Docker Content Trust's silent partner, you can also work with with it directly from the command line. For more complicated cases - especially relating to the administration of Content Trust servers - you'll probably have no choice but to dig deep into Notary's functionality and make a study of it. I'm not going to go there in this course, but I will just note that the Notary CLI is available through regular package managers like apt. 
You can use the CLI to learn about existing images - both local and remote. This "list" command will display the hashes from the Alpine image on Docker hub. Besides listing hashes, you can also initialize, add files to, and publish your own collections. 

As I mentioned away back at the beginning of the course, Docker Cloud offers a different approach to administrating your Docker resources. Well, different, but not too different. In fact, there does seem to be some overlap between Docker Cloud and Docker Datacenter, and that may be because Docker Cloud - originally called Tutum - was actually adopted by Docker when it was already a fairly mature product. So it's possible that Datacenter and Cloud evolved in parallel. Either way, Cloud is definitely an important tool for integrating your entire code-to-production workflow, so I'm going to spend a few minutes introducing you to it. 
In this demo, I'm going to show you around the Docker Cloud GUI, install and work with the Docker Cloud command line interface, and then briefly explain the functions of stacks, services, and containers.
We'll begin at the beginning: the Docker Cloud home page. You will notice that I'm automatically logged in. My Docker Hub account covers me here, as well. The first link in the left panel is to the Cloud Registry (or, as it's called here, Repositories). I'll click on Create and...create a new repo. There's not much to this, just give it a name and, if you want, a description and decide whether it should be public or private. So far it looks suspiciously like Docker Hub. In fact, I strongly suspect this is just a different front end to the same service. One thing that you don't get over at Hub is the option of making this repo a target for code builds in your code repository. I haven't currently got any associated repos to make that work.
Great. We've got ourselves a repo. Now what? Well, I think it's about time I told you about the Docker Cloud CLI. It works with the regular Docker Engine and, if you're working from a shell that's logged into Hub, you're also logged into Cloud. But it installs through Python PIP. I'll install pip and, just because it's the right thing to do, upgrade it to the latest version. I'm obviously skipping through some of the really fun parts where I get to watch the packages downloading. When that's done, I'll use pip to install docker-cloud.
Let's see what we can do with this new toy. The CLI works much the same way as Docker Engine, with the obvious difference that you start with the prefix docker-cloud rather than docker. --help gives us an overview of top level commands. "Docker-cloud repository inspect" will tell me about the myrepo I just created. Not a lot going on there, especially considering as I haven't yet added an image. 
I'll change that right away. For this step, I'll use the old Docker CLI to tag my busybox image with the name of my account and myrepo. Still with Docker, I'll push the image to the repo. Did it work? Why not see for ourselves by running repository inspect once again? 
The rest of what we're going to see goes a bit beyond the scope of a course on Docker images, but it's obviously important to know how all these parts fit together. The shiniest images in the repo won't do you much good if you don't know how to deploy them. The Stacks page allows you to upload or enter your own YAML scripts that launch integrated services. The sample code I'm pasting actually comes from Docker Cloud documentation and nicely illustrates a deployment that includes special Docker Cloud images of a load balancer based on HAProxy, the Docker Cloud quickstart-python webserver, and a Redis image. 
You can also push a stack file by way of the CLI.
Clicking Create on the Services page takes us to the first of three categories of infrastructure templates. The first category is called Jumpstarts, and includes pre-built frameworks like Redis, ElasticSearch, or MariaDB - on which I'll click. There's a lot of configuration you can do here, but, effectively, you've got the basic structure of a pretty sophisticated deployment just waiting for your final touches and the launch command. You can also search for images online and any images that might be associated with your Docker account. Once a service is actually started, you can track and manage it from the Containers page. There's nothing running right now. 
We've got just one more stop before heading for the exit: Cloud Settings. Here's where you can enter authentication data for your cloud platform or source code management accounts. This means that using Docker Cloud to pipe code and administrate your AWS resources is pretty much trivial. Of course, so is spending lots of money running lots of cloud resources...so do remember to properly monitor whatever cloud stuff you start up.
It's time for some review. We learned how Docker Content Trust uses Notary to protect images in transit through the use of encrypted signatures and keys. You enable DCT by exporting it to your environment with a value of 1. Once enabled, DCT will prevent you from pulling unsigned images and require authentication before allowing you to push. Notary can also be installed and used directly from the command line.
Docker Cloud has its own image repository, sometimes called Cloud Registry. The Docker Cloud CLI is installed through Python pip and can manage a wide range of administration duties remotely. Finally, the Docker Cloud browser interface is organized by Stack (where you can upload yaml scripts), Services (where complex deployments are defined), and Containers (where running containers are managed).

That's the end of the line for this course. Before we go, why not take a short look at some of the key things we saw. We learned about the way Docker images are organized by layers, with the operating system, applications, and data each on its own, and a separate layer that's writable to the container at run time. We re-introduced ourselves to the core Docker CLI tool, Docker Engine, and quickly got up to speed on building and managing images from both the command line and dockerfiles. And we mentioned some of the key image management tools, including Docker Trusted Registry and Docker Registry.
We also saw how each layer of an image can be shared among multiple images and containers, making downloads and launches much quicker - playing a big part in the overall Docker value proposition.
We learned how to search for and build images, and how to dig into the history and structure of the images we've already got, and we spent some time exploring some the best practices for building images - including convincingly demonstrating that extra layers can often translate to extra bloat. 
Turning our attention to Docker Registry, we learned how to get the free private repository tool running both through a package manager and using an purpose-built image. We pushed and pulled images to and from our local registry, and saw where all the repo data is actually hosted on the server file system. 
Security should always be a prominent part of any resource deployment, and Docker images are no exception. So we learned how incorporate encryption certificates into our Docker Registry, either by adding them to the registry container at run time, through the dockerfile, or as part of the config.yml file. We added the use of a signed key infrastructure to the mix to allow trust between image publishers and consumers. And, finally, we toured briefly through the Docker Cloud resource management interface. 
All in all, I was excited about some of the power and functionality I've seen in Docker images, and I hope that you've come away from this course with at least as much knowledge and respect for the tools as I have. Don't forget to make use of the Docker Images lab setup page I created over at bootstrap-it.com/docker-images. I hope to see you all around.

