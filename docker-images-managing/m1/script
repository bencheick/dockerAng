Introduction to Docker Images
Hi. I'm here to talk about Docker images. Along with other container technologies, Docker has significantly changed the way many public-facing application services are provided. Containers are fast, extremely versatile, and incredibly good at squeezing every last ounce of value out of your physical hardware. Since so much of the Docker advantage hangs on the proper design and creation of images, this is one part of the technology you'll really want to understand well. So stick around.
I'm going to assume that you're already familiar with the basics of installing Docker on a workstation or server and firing up at least a simple container. If you're not so sure you're quite there yet, no problem. There are a few courses here on Pluralsight that can quickly bring you up to speed. One good place to get caught up is my own Using Docker on AWS course. 
Now, about this course; here's what's coming. I plan to introduce you to the basic structure and function of images within the broader Docker ecosystem, and at least briefly touch on the various services currently available for managing images and making them available so Docker can do all the Dockery stuff that'll prove so useful. I'll show you how to acquire, build, and share images, and talk about Docker image best practices. We'll build our own Docker Registry - a locally hosted repository for Docker images so that teams working within a private network can efficiently share images with each other. We'll also learn about how images can be secured in transit, so we don't end up pulling or pushing images that have been maliciously compromised. And finally, we'll go just a bit deeper into a couple of newer image administration tools like Docker Cloud.
Now, let me describe my work environment just a bit. I'll be working from a terminal on a virtual instance of an Ubuntu 16.04 Linux machine running in VirtualBox. But that's not nearly as significant as you might think. In fact, Docker now runs happily on any of the three big operating systems. So once you get it nicely installed, the Docker Engine will work pretty much exactly the same way no matter where you're using it. If you don't mind though, there is one small Ubuntu trick that I will use. By default, running Docker commands from a terminal requires administrator privileges...which can be inconvenient over long stretches of time, or insecure...or both. So I'm going to edit the /etc/group file and add my user - who happens to be named "ubuntu" - to the docker group. From now on, my regular user will have automatic access to Docker-related resources on my system.
By the way, I've created a page for the course on my boostrap-it.com website - it's at bootstrap-it.com/docker-images. This page has all the lab setup information, configuration and script files, along with the many commands we'll be using throughout this course. Please use the page as an easy resource to avoid having to type everything out yourself, and also as a review tool.
One of the things I added to the page is a simple script you can use to really quickly install Docker Engine on to an Ubuntu or Debian machine. If I'm using a cloned VirtualBox VM, the script can get me from zero to a fully provisioned Docker machine in just a bit more than two minutes. 
We're all set to start. But first, a simple question: just what is an image? It's a software file containing a snapshot of a full filesystem. Everything necessary to launch a viable virtual server is included. An image might consist of just a base operating system like Ubuntu Linux, or the tiny and super-fast Alpine Linux. But an image could also include additional layers with software applications like web servers and databases. No matter how many layers an image has and how complicated the relationships between them might be, the image itself never changes. 
When in image is launched as a container, an extra writable layer is automatically added into which the record of any ongoing system activity is saved.
In this clip we're going to discuss some image management services. It's nice that the Docker toolbox is full to overflowing...giving us all kinds of choices. To some degree, however, distinguishing between the many choices and figuring out which is best for your needs can be a bit confusing. I'll try to clear things up at least a bit.
When you install Docker on your Linux, Mac, or Windows workstation - or on a cloud-based server, for that matter - the key to it all is Docker Engine. Docker Engine negotiates for access to shared hardware resources with the operating system kernel and provides a daemon through which you and launch, manage, and shut down containers. While we'll soon see how a number of external tools exist to help you visualize or abstract some of your more complicated Docker tasks, the truth is that there's really very little you can't do using only the Docker Engine.
The first place you're likely to come face to face with Docker Engine is on the command line, when it responds to commands beginning with "docker..." - like "docker images" that will list all of the images currently available on your local system. The ways you build or download, and then consume images are all managed by Docker Engine. 
At least on Linux systems, important data is kept in the directory /var/lib/docker. Drilling down a couple of levels to the layers directory shows us existing image layers represented by these hashes. It's Docker's storage drivers - like the AUFS unification filesystem - that provide the organization allowing layers from multiple sources to be "stacked up" and utilized as though they're all part of a single object. This is how Docker can instantly know exactly what's already available locally that can be put to work on new projects without the need to download or load the same objects over and over again. Whether or not you'll notice the difference right away, the time and resources you will save over the long term thanks to this architecture will be significant. 
You can think of Docker Hub as a GUI front end to the docker push and pull commands you've run from the command line on your local workstation. Besides creating and deleting repositories and managing which users can access your images, there really isn't all that much you can do on the Docker Hub site. Besides create and manage organizations, that is. Oh. And research, too. Clicking on the Explore link at the top of the page presents you with a couple of pages of the most popular official repositories, including the Ngninx web server running on Alpine Linux, Ubuntu, and Docker Registry - for building your own image repository environments within a Docker container on your own local server. We'll definitely come back to this one later in the course.
I'll click on a repo - Busybox, which is so small it makes Alpine Linux seem bloated - where we're given the docker pull command we should use from our local Docker Engine to get the image. A bit further down the page we get a description of what BusyBox is and what kinds of things you can do with it, and then some sample commands and dockerfile elements. 
One more thing about Docker Hub. You're allowed an unlimited number of public repositories for free. But, since many projects require at least some protection for your intellectual property, most people will eventually require more than the single private repo you're allowed on the free tier. Extra private repos are billed by volume. Clicking on the "Get more" link at the top of the screen will show you how much financial punishment you're likely to face. .
Docker Cloud is a resource administration service that you manage through your browser. What Docker Cloud does do is efficiently and visually coordinate all the resources you want to run. 
So you can store your images on Docker Hub, within a Docker Registry, or on your own Docker host running within your local infrastructure (using their "Bring Your Own Node" feature). 
Similarly, you tell Docker Cloud where you want your containers to run - whether it's your Amazon Web Services, Azure, or Digital Ocean account, or your own workstation - assuming that it's got Docker Engine installed. 
The idea is that you install a Cloud agent on whatever you're using for a host node, and the agent will act as a control interface, carrying out any instructions it gets from Docker Cloud. The agent actually runs as a number of Docker containers...but you never need to actually see all that if you don't want to. From now on, you only need to deal with the browser console, using it to select images and deploy them to deliver services through node clusters. We'll see how all this works in a later module.

The Docker Cloud model is quite similar to Amazon's EC2 Container Service. ECS itself is really nothing but an administration interface that coordinates images stored either in Amazon's own EC2 Container Registry or in Docker Hub, and launches them to an EC2 virtual machine instance that has the ECS agent installed. I would say that the two key differences between the Docker Cloud and ECS are that Amazon's service, obviously, is focused more narrowly on integrating with AWS-based resources, and that the ECS paradigm is a great deal more difficult to understand. But then, I guess I have an interest in saying that, as it will make it all that more likely that, just as soon as you're finished this one, you'll run right over and take my Using Docker on AWS course, which spends most of its time on ECS.

Just two more services to discuss: Docker Trusted Registry and Docker Registry. Docker Trusted Registry (DTR) is like the Docker Engine you run on your own workstation, in that it can be used to store and manage your images. However, in two ways, it's much bigger than that. First of all, DTR is designed to be deployed as a network server, much like Docker Hub, but under your complete control. And secondly, it's closely integrated with Docker Datacenter. Datacenter, like Docker Cloud, is a browser-based administration tool for orchestrating complex combinations of container resources. Unlike Docker Cloud, Datacenter must be hosted on your infrastructure - whether on a local server or within a cloud platform like AWS. 

Both Docker Trusted Registry and Datacenter are commercial enterprise products. I'm not going to spend time in this course discussing the Docker Trusted Registry because that's already been nicely handled by Elton Stoneman in his Getting Started with Docker Datacenter course here on Pluralsight. But I do plan to spend a lot more time on Docker Registry (without the "Trusted"). 

Docker Registry is a free toolset that you install locally to create and manage your own images in a private environment. It doesn't have the GUI or the technical support that comes with Trusted Registry, but it's got everything a DevOps operation might need to manage images within a private network, support a continuous integration/deployment workflow, or deploy a cluster of nodes built on your customized image.

That's it for our Docker images introduction. It's time for a bit of a review of what we've seen so far. We saw that docker images will list all images available locally, docker build will create an image based on a dockerfile script, and Docker data is kept in the /var/lib/docker directory. Docker Hub is a public repository of images, both official, vendor-supported images, and others built by people like you. Docker Cloud is a hosted, browser-based administration interface for managing your own Docker-related resources. Amazon's EC2 Container Registry is a repo where you can store your own images that's closely integrated with Amazon's Docker host, EC2 Container Service. The Docker Trusted Registry is Docker's own premium image repository. Trusted Registry is part of the Docker's enterprise service, Docker Datacenter. Finally, Docker Registry is Docker's open source repository that's designed for active private network deployments.
In the next module, we'll play around with Docker Engine to pull images from Docker Hub, create a few of our own images using dockerfiles, and explore some best practices used to get the most out of your images.

